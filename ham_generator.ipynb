{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ham_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqi7zJcqJWuR"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw4b21_bMttQ"
      },
      "source": [
        "import re"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE33oe-rJbTz"
      },
      "source": [
        "PAGE_LINK = 'https://www.allmusicals.com/h/hamilton.htm'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jKLNv0iJdeQ",
        "outputId": "d9a381ae-ed8a-4506-b4f4-e6110e93e9fb"
      },
      "source": [
        "page = requests.get(PAGE_LINK)\n",
        "page.status_code"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNDyCcKTJp-g"
      },
      "source": [
        "soup = BeautifulSoup(page.text, \"html.parser\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nIL1QodJtpn"
      },
      "source": [
        "lyrics_list = [a['href'] for a in soup.find_all(\"section\", {\"class\": \"lyrics-list\"})[0].find_all('a', href=True)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmsLVBcMMWn"
      },
      "source": [
        "all_lyrics = ''\n",
        "for link in lyrics_list:\n",
        "  temp = requests.get(link)\n",
        "  lyrics = BeautifulSoup(temp.text, \"html.parser\").find('div', {'id': \"page\"})\n",
        "  all_lyrics += lyrics.text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICThYqI0My2o"
      },
      "source": [
        "all_lyrics_clean = all_lyrics.replace('\\t', '').replace('\\n', '')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3awOraOANbHt"
      },
      "source": [
        "#remove brackets who is speaking\n",
        "all_lyrics_clean = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", all_lyrics_clean)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ7ZJysZNzva"
      },
      "source": [
        "format_title_indices = [m.start() for m in re.finditer('Last Update', all_lyrics_clean)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f91g7hoOxuE"
      },
      "source": [
        "substr_to_remove = []\n",
        "for idx in format_title_indices:\n",
        "  temp = all_lyrics_clean[idx: idx+100]\n",
        "  space_idx = temp.find('   ')\n",
        "  substr_to_remove.append(temp[: space_idx])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MugTzBBXVD0r"
      },
      "source": [
        "#remove dates and titles form text\n",
        "for s in substr_to_remove:\n",
        "  all_lyrics_clean = all_lyrics_clean.replace(s, '')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxYe4T9rW14g"
      },
      "source": [
        "#clean multiple spaces\n",
        "all_lyrics_clean = re.sub(' +', ' ', all_lyrics_clean)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RARLJsR3NfO6"
      },
      "source": [
        "#clean wrong punctuation\n",
        "all_lyrics_clean = all_lyrics_clean.replace(\"?s\", \"'s\").replace(\"?ll\", \"'ll\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVfSL8ZXa9vv"
      },
      "source": [
        "all_lyrics_list = [line for line in all_lyrics_clean.split('\\r') if line!='']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZAzcLHaXRMk",
        "outputId": "c1c7131c-f8e5-4704-cc75-925637b8a69a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iELHH-O6bQAP"
      },
      "source": [
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/ham_lyrics_list.pkl', 'wb') as f:\n",
        "  pickle.dump(all_lyrics_list, f)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYvQs3zsZ34m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jKbBlHVZ31-"
      },
      "source": [
        "train, val = train_test_split(all_lyrics_list, test_size = 0.1, shuffle=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrClx8tHbowN"
      },
      "source": [
        "with open('../../../../drive/MyDrive/Colab Notebooks/train_ham2.txt', 'w') as f:\n",
        "  f.write(\"\".join(train))\n",
        "\n",
        "with open('../../../../drive/MyDrive/Colab Notebooks/eval_ham2.txt', 'w') as f:\n",
        "  f.write(\"\".join(val))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85z3z19rdAYX",
        "outputId": "26fbb13b-ac90-4ecf-b9ee-5d7fb7703832"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-razitiaf\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-razitiaf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.9.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2.10)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.9.0.dev0-cp37-none-any.whl size=2555688 sha256=713720eaa2d43fde8e21c05adfaa6d1db694cbfa60c49f4569102aec7c88d606\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jy75zkqy/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfe_-_Q2cJQQ",
        "outputId": "130d9174-c04e-44fc-e81c-cb80cae7d21b"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 77671, done.\u001b[K\n",
            "remote: Counting objects: 100% (716/716), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 77671 (delta 427), reused 528 (delta 288), pack-reused 76955\u001b[K\n",
            "Receiving objects: 100% (77671/77671), 59.87 MiB | 10.44 MiB/s, done.\n",
            "Resolving deltas: 100% (55374/55374), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIl1BTnFcJOn",
        "outputId": "36859133-cd24-47b2-cfc4-d0279b46b486"
      },
      "source": [
        "import os\n",
        "os.chdir(\"transformers\")\n",
        "os.chdir(\"./examples/pytorch/language-modeling\")\n",
        "!ls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md\t  run_clm_no_trainer.py  run_mlm_no_trainer.py\trun_plm.py\n",
            "requirements.txt  run_clm.py\t\t run_mlm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ77Ww3_cJLc",
        "outputId": "9ed602e0-aca2-471d-bd6c-e5a5fd789c8b"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.9.0+cu102)\n",
            "Collecting datasets>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/27/9c91ddee87b06d2de12f134c5171a49890427e398389f07f6463485723c3/datasets-1.9.0-py3-none-any.whl (262kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 4.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Collecting fsspec>=2021.05.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (20.9)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.0.12)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.0.12)\n",
            "Installing collected packages: fsspec, xxhash, datasets, sentencepiece\n",
            "Successfully installed datasets-1.9.0 fsspec-2021.6.1 sentencepiece-0.1.96 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar_A4Y_CYOvg",
        "outputId": "fdaf2f8e-495f-403e-c270-5e3e35ad943f"
      },
      "source": [
        "!python run_clm.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path gpt2 \\\n",
        "--train_file \"../../../../drive/MyDrive/Colab Notebooks/train_ham2.txt\" \\\n",
        "--do_train \\\n",
        "--validation_file \"../../../../drive/MyDrive/Colab Notebooks/eval_ham2.txt\" \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--fp16 \\\n",
        "--output_dir=\"../../../../drive/MyDrive/Colab Notebooks/ham_generator2\" \\\n",
        "--overwrite_output_dir True"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-09 09:29:11.758951: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/09/2021 09:29:13 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/09/2021 09:29:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../../../drive/MyDrive/Colab Notebooks/ham_generator2/runs/Jul09_09-29-13_f5c55e2fb4ae,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=../../../../drive/MyDrive/Colab Notebooks/ham_generator2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=ham_generator2,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../../../drive/MyDrive/Colab Notebooks/ham_generator2,\n",
            "save_on_each_node=False,\n",
            "save_steps=-1,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/09/2021 09:29:15 - WARNING - datasets.builder - Using custom data configuration default-a1d1932b1767bf59\n",
            "07/09/2021 09:29:15 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "100% 2/2 [00:00<00:00, 2305.20it/s]\n",
            "07/09/2021 09:29:15 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "07/09/2021 09:29:15 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 101.01it/s]\n",
            "07/09/2021 09:29:15 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/09/2021 09:29:15 - INFO - datasets.builder - Generating split train\n",
            "07/09/2021 09:29:15 - INFO - datasets.builder - Generating split validation\n",
            "07/09/2021 09:29:15 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 814.98it/s]\n",
            "[INFO|configuration_utils.py:537] 2021-07-09 09:29:16,404 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:573] 2021-07-09 09:29:16,405 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:432] 2021-07-09 09:29:17,120 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:537] 2021-07-09 09:29:17,837 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:573] 2021-07-09 09:29:17,837 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 09:29:22,400 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 09:29:22,400 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 09:29:22,400 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 09:29:22,400 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 09:29:22,400 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 09:29:22,400 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|modeling_utils.py:1237] 2021-07-09 09:29:23,190 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1448] 2021-07-09 09:29:25,212 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1457] 2021-07-09 09:29:25,213 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3193] 2021-07-09 09:29:25,319 >> Token indices sequence length is longer than the specified maximum sequence length for this model (25908 > 1024). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|run_clm.py:370] 2021-07-09 09:29:25,319 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
            "07/09/2021 09:29:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-1d41b88ad013821f.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 12.99ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/09/2021 09:29:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-591d92762f40352c.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 111.00ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/1 [00:00<?, ?ba/s]07/09/2021 09:29:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-c46cfc489c64b94c.arrow\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00, 38.44ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/1 [00:00<?, ?ba/s]07/09/2021 09:29:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-a1d1932b1767bf59/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-c2f4bf0084b006c7.arrow\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00, 210.19ba/s]\n",
            "[INFO|trainer.py:420] 2021-07-09 09:29:27,792 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:761] 2021-07-09 09:29:27,795 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:761] 2021-07-09 09:29:27,795 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1158] 2021-07-09 09:29:27,799 >> ***** Running training *****\n",
            "[INFO|trainer.py:1159] 2021-07-09 09:29:27,799 >>   Num examples = 25\n",
            "[INFO|trainer.py:1160] 2021-07-09 09:29:27,799 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1161] 2021-07-09 09:29:27,799 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1162] 2021-07-09 09:29:27,799 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1163] 2021-07-09 09:29:27,799 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1164] 2021-07-09 09:29:27,799 >>   Total optimization steps = 125\n",
            "[WARNING|training_args.py:761] 2021-07-09 09:29:27,814 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1304: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "100% 125/125 [02:22<00:00,  1.14s/it][INFO|trainer.py:1354] 2021-07-09 09:31:50,580 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7817, 'train_samples_per_second': 0.875, 'train_steps_per_second': 0.875, 'train_loss': 3.820642333984375, 'epoch': 5.0}\n",
            "100% 125/125 [02:22<00:00,  1.14s/it]\n",
            "[INFO|trainer.py:1913] 2021-07-09 09:31:50,585 >> Saving model checkpoint to ../../../../drive/MyDrive/Colab Notebooks/ham_generator2\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 09:31:50,591 >> Configuration saved in ../../../../drive/MyDrive/Colab Notebooks/ham_generator2/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 09:31:52,871 >> Model weights saved in ../../../../drive/MyDrive/Colab Notebooks/ham_generator2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 09:31:52,878 >> tokenizer config file saved in ../../../../drive/MyDrive/Colab Notebooks/ham_generator2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 09:31:52,882 >> Special tokens file saved in ../../../../drive/MyDrive/Colab Notebooks/ham_generator2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     3.8206\n",
            "  train_runtime            = 0:02:22.78\n",
            "  train_samples            =         25\n",
            "  train_samples_per_second =      0.875\n",
            "  train_steps_per_second   =      0.875\n",
            "07/09/2021 09:31:53 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2159] 2021-07-09 09:31:53,052 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2161] 2021-07-09 09:31:53,052 >>   Num examples = 2\n",
            "[INFO|trainer.py:2164] 2021-07-09 09:31:53,052 >>   Batch size = 8\n",
            "100% 1/1 [00:00<00:00,  2.38it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_loss               =     3.7902\n",
            "  eval_runtime            = 0:00:00.44\n",
            "  eval_samples            =          2\n",
            "  eval_samples_per_second =       4.52\n",
            "  eval_steps_per_second   =       2.26\n",
            "  perplexity              =    44.2634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTVloHWUcqaG",
        "outputId": "f38fdfb2-9346-4448-a25a-872262804c48"
      },
      "source": [
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"../../../../drive/MyDrive/Colab Notebooks/ham_generator2\", from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.9.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'lm_head.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.4.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3pGyKXfiQlV"
      },
      "source": [
        "input_ids = tokenizer.encode(\"wait for it\", return_tensors='tf')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4aYDfuIiYHP",
        "outputId": "ed43d0bd-031c-40d4-8c62-21210e9fcdf2"
      },
      "source": [
        "#https://huggingface.co/blog/how-to-generate\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=30, \n",
        "    min_length=10, \n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=3,\n",
        "    top_p=0.92,\n",
        "    temperature=.7,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Eqr8v0pidbQ",
        "outputId": "d8929254-2640-42a4-afe9-4c9c458aef41"
      },
      "source": [
        "for i, beam in enumerate(beam_output):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam)))\n",
        "  print()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: wait for it. I would rather be the King of England than be President of France. I am willing to do whatever it takes to make America great\n",
            "\n",
            "1: wait for it…The King of France, the British, the French, the Dutch, they all made a treaty of war. I am not saying\n",
            "\n",
            "2: wait for it, but we need to do it right now.\n",
            "\n",
            "The rest of the world is watching, watching us. It's happening all\n",
            "\n",
            "3: wait for it!\n",
            "\n",
            "I know it's crazy, but I'm gonna make it real…I'm gonna tell you what I saw.I\n",
            "\n",
            "4: wait for it. And I would like to be there. I was just so sorry I didn't get to see you, you know? I'm\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSjSEjqRiueM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}